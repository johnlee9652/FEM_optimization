{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64005cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import optimize\n",
    "import os\n",
    "import numpy as np\n",
    "import os.path\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4651158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    global count\n",
    "    # Ea, Ploading = x\n",
    "    Ploading, Ea = x\n",
    "    new_parameters = 'Ea = ' + str(Ea) + '\\n', 'Ploading = ' + str(Ploading) + '\\n'\n",
    "    with open(\"parameters.txt\", \"w\") as file:\n",
    "        file.writelines(new_parameters)\n",
    "    with open(\"parameters_archive.txt\", \"a\") as file:\n",
    "        file.writelines(str(count)+'\\t'+str(Ea)+'\\t'+str(Ploading)+'\\n')\n",
    "    \n",
    "    width = []\n",
    "    for dP in range(2,13):\n",
    "        # write input file\n",
    "        data = data2 = data3 = \"\"  \n",
    "        with open('header.txt') as fp:\n",
    "            data = fp.read()\n",
    "        with open('parameters.txt') as fp:\n",
    "            data2 = fp.read()\n",
    "        with open('body_'+str(dP)+'.txt') as fp:\n",
    "            data3 = fp.read()\n",
    "        data = data + data2 + data3\n",
    "        filename = 'extrusionLoadingPS_'+str(dP)+'_'+str(count)+'.inp'\n",
    "        with open (filename, 'w') as fp:\n",
    "            fp.write(data)\n",
    "   \n",
    "        # run abaqus\n",
    "        os.system('L:;cd EL_final_final/optimization_16')\n",
    "        os.system('abaqus job=' + filename + ' cpus=3')\n",
    "    \n",
    "        time.sleep(10)\n",
    "    \n",
    "        file_path = 'L:/EL_final_final/optimization_4/extrusionLoadingPS_%s.lck'%(str(dP)+'_'+str(count))\n",
    "        while os.path.exists(file_path):\n",
    "            time.sleep(300)\n",
    "    \n",
    "        # modify macro file\n",
    "        pathName = \"pathName = 'L:/EL_final_final/optimization_4/extrusionLoadingPS_%s.odb'\"%(str(dP)+'_'+str(count))\n",
    "        with open(\"macro_path.txt\", \"w\") as file:\n",
    "            file.writelines(pathName)    \n",
    "        data = data2 = data3 = \"\"  \n",
    "        with open('macro_header.txt') as fp:\n",
    "            data = fp.read()\n",
    "        with open('macro_path.txt') as fp:\n",
    "            data2 = fp.read()\n",
    "        with open('macro_body.txt') as fp:\n",
    "            data3 = fp.read()\n",
    "        data = data + '\\n' + data2 + '\\n' + data3\n",
    "        filename = 'getWidth.py'\n",
    "        with open (filename, 'w') as fp:\n",
    "            fp.write(data)\n",
    "\n",
    "        # output report file from abaqus\n",
    "        os.system('abaqus cae noGUI=getWidth.py')\n",
    "\n",
    "        # read report file and load deformed width\n",
    "        with open('abaqus.rpt') as file:\n",
    "            data = file.readlines()[-5]\n",
    "        width = width + [float(data.split()[1]) * 2]\n",
    "    \n",
    "    # get mse\n",
    "    true_width = [0.8795,0.8777,0.8362,0.7849,0.7417,0.7012]\n",
    "    diff = [a - b for a, b in zip(true_width,width)]\n",
    "    se = [d**2 for d in diff]\n",
    "    se = np.array(se)\n",
    "    mse = np.mean(se)\n",
    "    \n",
    "    with open(\"mse_archive.txt\", \"a\") as file:\n",
    "        file.writelines(str(count)+'\\t'+str(mse)+'\\n')\n",
    "    \n",
    "    # count up simulation\n",
    "    count += 1\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate text archive files containing parameter values and mse values\n",
    "with open('parameters_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'Ea'+'\\t'+'Ploading'+'\\n')\n",
    "with open('mse_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'MSE'+'\\n')\n",
    "\n",
    "# initialize count variable to count up number of parameter sets\n",
    "count = 1\n",
    "\n",
    "itr = 1\n",
    "def callbackF(x):\n",
    "    global itr\n",
    "    print('##################################################################',count,x[0],x[1])\n",
    "    itr += 1\n",
    "\n",
    "from scipy.optimize import LinearConstraint\n",
    "cons = scipy.optimize.LinearConstraint((1,-0.022),0,np.inf)\n",
    "\n",
    "# call optimization function\n",
    "scipy.optimize.minimize(fun, x0=[0.2,3], method='SLSQP', bounds=((0,0.6),(0,20)), constraints=(cons), callback=callbackF, options = {'iprint':101, 'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e1c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    global count\n",
    "    Ploading, Ea = x\n",
    "    Ploading = Ploading / 1000\n",
    "    new_parameters = 'Ea = ' + str(Ea) + '\\n', 'Ploading = ' + str(Ploading) + '\\n'\n",
    "    with open(\"parameters.txt\", \"w\") as file:\n",
    "        file.writelines(new_parameters)\n",
    "    with open(\"parameters_archive.txt\", \"a\") as file:\n",
    "        file.writelines(str(count)+'\\t'+str(Ea)+'\\t'+str(Ploading)+'\\n')\n",
    "    \n",
    "    width = []\n",
    "    for dP in range(2,13):\n",
    "        # write input file\n",
    "        data = data2 = data3 = \"\"  \n",
    "        with open('header.txt') as fp:\n",
    "            data = fp.read()\n",
    "        with open('parameters.txt') as fp:\n",
    "            data2 = fp.read()\n",
    "        with open('body_'+str(dP)+'.txt') as fp:\n",
    "            data3 = fp.read()\n",
    "        data = data + data2 + data3\n",
    "        filename = 'extrusionLoadingPS_'+str(dP)+'_'+str(count)+'.inp'\n",
    "        with open (filename, 'w') as fp:\n",
    "            fp.write(data)\n",
    "   \n",
    "        # run abaqus\n",
    "        os.system('L:;cd EL_final_final/optimization_16')\n",
    "        if count <= 30:\n",
    "            pass\n",
    "        #elif count == 20 and dP <= 7:\n",
    "        #    pass\n",
    "        else:\n",
    "            os.system('abaqus job=' + filename + ' cpus=3')\n",
    "    \n",
    "        time.sleep(10)\n",
    "    \n",
    "        file_path = 'L:/EL_final_final/optimization_16/extrusionLoadingPS_%s.lck'%(str(dP)+'_'+str(count))\n",
    "        while os.path.exists(file_path):\n",
    "            time.sleep(180)\n",
    "    \n",
    "        # modify macro file\n",
    "        pathName = \"pathName = 'L:/EL_final_final/optimization_16/extrusionLoadingPS_%s.odb'\"%(str(dP)+'_'+str(count))\n",
    "        with open(\"macro_path.txt\", \"w\") as file:\n",
    "            file.writelines(pathName)    \n",
    "        data = data2 = data3 = \"\"  \n",
    "        with open('macro_header.txt') as fp:\n",
    "            data = fp.read()\n",
    "        with open('macro_path.txt') as fp:\n",
    "            data2 = fp.read()\n",
    "        with open('macro_body.txt') as fp:\n",
    "            data3 = fp.read()\n",
    "        data = data + '\\n' + data2 + '\\n' + data3\n",
    "        filename = 'getWidth.py'\n",
    "        with open (filename, 'w') as fp:\n",
    "            fp.write(data)\n",
    "\n",
    "        # output report file from abaqus\n",
    "        os.system('abaqus cae noGUI=getWidth.py')\n",
    "\n",
    "        # read report file and load deformed width\n",
    "        with open('abaqus.rpt') as file:\n",
    "            data = file.readlines()[-5]\n",
    "        width = width + [float(data.split()[1]) * 2]\n",
    "    \n",
    "    # residual\n",
    "    true_width = [0.8831,0.8690,0.8670,0.7724,0.8002,0.7394,0.6468,0.5993,0.5842,0.5411,0.5351]\n",
    "    diff = [a - b for a, b in zip(true_width,width)]\n",
    "    residuals = np.array(diff)\n",
    "    width_report = np.array(width)\n",
    "    \n",
    "    with open(\"residuals_archive.txt\", \"a\") as file:\n",
    "        if count == 1:\n",
    "            file.writelines(str(count)+'\\t')\n",
    "        else:\n",
    "            file.writelines('\\n'+str(count)+'\\t')\n",
    "        for res in residuals:\n",
    "            file.writelines(str(res)+'\\t')\n",
    "    \n",
    "    with open(\"width_archive.txt\", \"a\") as file:\n",
    "        if count == 1:\n",
    "            file.writelines(str(count)+'\\t')\n",
    "        else:\n",
    "            file.writelines('\\n'+str(count)+'\\t')\n",
    "        for w in width_report:\n",
    "            file.writelines(str(w)+'\\t')\n",
    "            \n",
    "    # count up simulation\n",
    "    count += 1\n",
    "    \n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ecbef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         1.9524e-01                                    1.89e-01    \n",
      "       1              2         1.2387e-01      7.14e-02       2.72e+02       4.04e-02    \n",
      "       2              3         4.2019e-02      8.19e-02       1.79e+02       2.45e+00    \n",
      "       3              4         1.9355e-02      2.27e-02       2.86e+01       1.22e+00    \n",
      "       4              5         5.9590e-03      1.34e-02       3.41e+00       3.74e-02    \n",
      "       5             11         5.9570e-03      1.94e-06       1.18e-04       3.78e-02    \n",
      "       6             13         5.9569e-03      1.26e-07       6.30e-05       3.77e-02    \n",
      "       7             18         5.9569e-03      0.00e+00       0.00e+00       3.77e-02    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 18, initial cost 1.9524e-01, final cost 5.9569e-03, first-order optimality 3.77e-02.\n"
     ]
    }
   ],
   "source": [
    "# initiate text archive files containing parameter values and mse values\n",
    "with open('parameters_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'Ea'+'\\t'+'Ploading'+'\\n')\n",
    "with open('residuals_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'residuals'+'\\n')\n",
    "\n",
    "# initialize count variable to count up number of parameter sets\n",
    "count = 1\n",
    "\n",
    "# call optimization function\n",
    "res = scipy.optimize.least_squares(fun, x0=(500,20), bounds=([0,0],[1000,20]), gtol=1e-02, diff_step=0.05, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72046cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " active_mask: array([0, 0])\n",
       "        cost: 0.005956914729999996\n",
       "         fun: array([-0.04704 ,  0.019112,  0.043054, -0.023084,  0.047338,  0.02694 ,\n",
       "       -0.0265  , -0.036554, -0.016678, -0.036268, -0.018726])\n",
       "        grad: array([-4.08386617e-05, -7.73647200e-05])\n",
       "         jac: array([[-0.00955145,  0.26052   ],\n",
       "       [-0.01657417,  0.06444   ],\n",
       "       [-0.00333277,  0.05108   ],\n",
       "       [-0.00316394,  0.033     ],\n",
       "       [-0.00590476,  0.01612   ],\n",
       "       [-0.00331958, -0.01516   ],\n",
       "       [-0.00270705, -0.04076   ],\n",
       "       [-0.00221059, -0.06012   ],\n",
       "       [-0.00181543, -0.0744    ],\n",
       "       [-0.00158382, -0.08212   ],\n",
       "       [-0.00137595, -0.08848   ]])\n",
       "     message: '`xtol` termination condition is satisfied.'\n",
       "        nfev: 18\n",
       "        njev: 7\n",
       "  optimality: 0.037742407179700524\n",
       "      status: 3\n",
       "     success: True\n",
       "           x: array([75.81674884,  0.85862638])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4521ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unified interfaces to minimization algorithms.\n",
    "\n",
    "Functions\n",
    "---------\n",
    "- minimize : minimization of a function of several variables.\n",
    "- minimize_scalar : minimization of a function of one variable.\n",
    "\"\"\"\n",
    "\n",
    "__all__ = ['minimize', 'minimize_scalar']\n",
    "\n",
    "\n",
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# unconstrained minimization\n",
    "from optimize_JL import (_minimize_neldermead, _minimize_powell, _minimize_cg,\n",
    "                       _minimize_bfgs, _minimize_newtoncg,\n",
    "                       _minimize_scalar_brent, _minimize_scalar_bounded,\n",
    "                       _minimize_scalar_golden, MemoizeJac)\n",
    "from scipy.optimize._trustregion_dogleg import _minimize_dogleg\n",
    "from scipy.optimize._trustregion_ncg import _minimize_trust_ncg\n",
    "from scipy.optimize._trustregion_krylov import _minimize_trust_krylov\n",
    "from scipy.optimize._trustregion_exact import _minimize_trustregion_exact\n",
    "from scipy.optimize._trustregion_constr import _minimize_trustregion_constr\n",
    "\n",
    "# constrained minimization\n",
    "from scipy.optimize.lbfgsb import _minimize_lbfgsb\n",
    "from scipy.optimize.tnc import _minimize_tnc\n",
    "from scipy.optimize.cobyla import _minimize_cobyla\n",
    "from scipy.optimize.slsqp import _minimize_slsqp\n",
    "from scipy.optimize._constraints import (old_bound_to_new, new_bounds_to_old,\n",
    "                           old_constraint_to_new, new_constraint_to_old,\n",
    "                           NonlinearConstraint, LinearConstraint, Bounds)\n",
    "from scipy.optimize._differentiable_functions import FD_METHODS\n",
    "\n",
    "MINIMIZE_METHODS = ['nelder-mead', 'powell', 'cg', 'bfgs', 'newton-cg',\n",
    "                    'l-bfgs-b', 'tnc', 'cobyla', 'slsqp', 'trust-constr',\n",
    "                    'dogleg', 'trust-ncg', 'trust-exact', 'trust-krylov']\n",
    "\n",
    "MINIMIZE_SCALAR_METHODS = ['brent', 'bounded', 'golden']\n",
    "\n",
    "def minimize(fun, x0, args=(), method=None, jac=None, hess=None,\n",
    "             hessp=None, bounds=None, constraints=(), tol=None,\n",
    "             callback=None, options=None):\n",
    "    \"\"\"Minimization of scalar function of one or more variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fun : callable\n",
    "        The objective function to be minimized.\n",
    "\n",
    "            ``fun(x, *args) -> float``\n",
    "\n",
    "        where ``x`` is an 1-D array with shape (n,) and ``args``\n",
    "        is a tuple of the fixed parameters needed to completely\n",
    "        specify the function.\n",
    "    x0 : ndarray, shape (n,)\n",
    "        Initial guess. Array of real elements of size (n,),\n",
    "        where 'n' is the number of independent variables.\n",
    "    args : tuple, optional\n",
    "        Extra arguments passed to the objective function and its\n",
    "        derivatives (`fun`, `jac` and `hess` functions).\n",
    "    method : str or callable, optional\n",
    "        Type of solver.  Should be one of\n",
    "\n",
    "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
    "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
    "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
    "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
    "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
    "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
    "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
    "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
    "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
    "            - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
    "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
    "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
    "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
    "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
    "            - custom - a callable object (added in version 0.14.0),\n",
    "              see below for description.\n",
    "\n",
    "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
    "        depending if the problem has constraints or bounds.\n",
    "    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
    "        Method for computing the gradient vector. Only for CG, BFGS,\n",
    "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
    "        trust-exact and trust-constr.\n",
    "        If it is a callable, it should be a function that returns the gradient\n",
    "        vector:\n",
    "\n",
    "            ``jac(x, *args) -> array_like, shape (n,)``\n",
    "\n",
    "        where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
    "        the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
    "        assumed to return a tuple ``(f, g)`` containing the objective\n",
    "        function and the gradient.\n",
    "        Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
    "        'trust-krylov' require that either a callable be supplied, or that\n",
    "        `fun` return the objective and gradient.\n",
    "        If None or False, the gradient will be estimated using 2-point finite\n",
    "        difference estimation with an absolute step size.\n",
    "        Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
    "        to select a finite difference scheme for numerical estimation of the\n",
    "        gradient with a relative step size. These finite difference schemes\n",
    "        obey any specified `bounds`.\n",
    "    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
    "        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
    "        trust-ncg, trust-krylov, trust-exact and trust-constr. If it is\n",
    "        callable, it should return the Hessian matrix:\n",
    "\n",
    "            ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
    "\n",
    "        where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
    "        parameters. LinearOperator and sparse matrix returns are only allowed\n",
    "        for 'trust-constr' method. Alternatively, the keywords\n",
    "        {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
    "        for numerical estimation. Or, objects implementing the\n",
    "        `HessianUpdateStrategy` interface can be used to approximate\n",
    "        the Hessian. Available quasi-Newton methods implementing\n",
    "        this interface are:\n",
    "\n",
    "            - `BFGS`;\n",
    "            - `SR1`.\n",
    "\n",
    "        Whenever the gradient is estimated via finite-differences,\n",
    "        the Hessian cannot be estimated with options\n",
    "        {'2-point', '3-point', 'cs'} and needs to be\n",
    "        estimated using one of the quasi-Newton strategies.\n",
    "        'trust-exact' cannot use a finite-difference scheme, and must be used\n",
    "        with a callable returning an (n, n) array.\n",
    "    hessp : callable, optional\n",
    "        Hessian of objective function times an arbitrary vector p. Only for\n",
    "        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
    "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
    "        provided, then `hessp` will be ignored.  `hessp` must compute the\n",
    "        Hessian times an arbitrary vector:\n",
    "\n",
    "            ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
    "\n",
    "        where x is a (n,) ndarray, p is an arbitrary vector with\n",
    "        dimension (n,) and `args` is a tuple with the fixed\n",
    "        parameters.\n",
    "    bounds : sequence or `Bounds`, optional\n",
    "        Bounds on variables for Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell, and\n",
    "        trust-constr methods. There are two ways to specify the bounds:\n",
    "\n",
    "            1. Instance of `Bounds` class.\n",
    "            2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
    "               is used to specify no bound.\n",
    "\n",
    "    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
    "        Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
    "\n",
    "        Constraints for 'trust-constr' are defined as a single object or a\n",
    "        list of objects specifying constraints to the optimization problem.\n",
    "        Available constraints are:\n",
    "\n",
    "            - `LinearConstraint`\n",
    "            - `NonlinearConstraint`\n",
    "\n",
    "        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
    "        Each dictionary with fields:\n",
    "\n",
    "            type : str\n",
    "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
    "            fun : callable\n",
    "                The function defining the constraint.\n",
    "            jac : callable, optional\n",
    "                The Jacobian of `fun` (only for SLSQP).\n",
    "            args : sequence, optional\n",
    "                Extra arguments to be passed to the function and Jacobian.\n",
    "\n",
    "        Equality constraint means that the constraint function result is to\n",
    "        be zero whereas inequality means that it is to be non-negative.\n",
    "        Note that COBYLA only supports inequality constraints.\n",
    "    tol : float, optional\n",
    "        Tolerance for termination. When `tol` is specified, the selected\n",
    "        minimization algorithm sets some relevant solver-specific tolerance(s)\n",
    "        equal to `tol`. For detailed control, use solver-specific\n",
    "        options.\n",
    "    options : dict, optional\n",
    "        A dictionary of solver options. All methods accept the following\n",
    "        generic options:\n",
    "\n",
    "            maxiter : int\n",
    "                Maximum number of iterations to perform. Depending on the\n",
    "                method each iteration may use several function evaluations.\n",
    "            disp : bool\n",
    "                Set to True to print convergence messages.\n",
    "\n",
    "        For method-specific options, see :func:`show_options()`.\n",
    "    callback : callable, optional\n",
    "        Called after each iteration. For 'trust-constr' it is a callable with\n",
    "        the signature:\n",
    "\n",
    "            ``callback(xk, OptimizeResult state) -> bool``\n",
    "\n",
    "        where ``xk`` is the current parameter vector. and ``state``\n",
    "        is an `OptimizeResult` object, with the same fields\n",
    "        as the ones from the return. If callback returns True\n",
    "        the algorithm execution is terminated.\n",
    "        For all the other methods, the signature is:\n",
    "\n",
    "            ``callback(xk)``\n",
    "\n",
    "        where ``xk`` is the current parameter vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : OptimizeResult\n",
    "        The optimization result represented as a ``OptimizeResult`` object.\n",
    "        Important attributes are: ``x`` the solution array, ``success`` a\n",
    "        Boolean flag indicating if the optimizer exited successfully and\n",
    "        ``message`` which describes the cause of the termination. See\n",
    "        `OptimizeResult` for a description of other attributes.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    minimize_scalar : Interface to minimization algorithms for scalar\n",
    "        univariate functions\n",
    "    show_options : Additional options accepted by the solvers\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This section describes the available solvers that can be selected by the\n",
    "    'method' parameter. The default method is *BFGS*.\n",
    "\n",
    "    **Unconstrained minimization**\n",
    "\n",
    "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
    "    gradient algorithm by Polak and Ribiere, a variant of the\n",
    "    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
    "    first derivatives are used.\n",
    "\n",
    "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
    "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
    "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
    "    performance even for non-smooth optimizations. This method also\n",
    "    returns an approximation of the Hessian inverse, stored as\n",
    "    `hess_inv` in the OptimizeResult object.\n",
    "\n",
    "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
    "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
    "    Newton method). It uses a CG method to the compute the search\n",
    "    direction. See also *TNC* method for a box-constrained\n",
    "    minimization with a similar algorithm. Suitable for large-scale\n",
    "    problems.\n",
    "\n",
    "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
    "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
    "    algorithm requires the gradient and Hessian; furthermore the\n",
    "    Hessian is required to be positive definite.\n",
    "\n",
    "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
    "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
    "    unconstrained minimization. This algorithm requires the gradient\n",
    "    and either the Hessian or a function that computes the product of\n",
    "    the Hessian with a given vector. Suitable for large-scale problems.\n",
    "\n",
    "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
    "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
    "    minimization. This algorithm requires the gradient\n",
    "    and either the Hessian or a function that computes the product of\n",
    "    the Hessian with a given vector. Suitable for large-scale problems.\n",
    "    On indefinite problems it requires usually less iterations than the\n",
    "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
    "\n",
    "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
    "    is a trust-region method for unconstrained minimization in which\n",
    "    quadratic subproblems are solved almost exactly [13]_. This\n",
    "    algorithm requires the gradient and the Hessian (which is\n",
    "    *not* required to be positive definite). It is, in many\n",
    "    situations, the Newton method to converge in fewer iteraction\n",
    "    and the most recommended for small and medium-size problems.\n",
    "\n",
    "    **Bound-Constrained minimization**\n",
    "\n",
    "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
    "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
    "    applications. However, if numerical computation of derivative can be\n",
    "    trusted, other algorithms using the first and/or second derivatives\n",
    "    information might be preferred for their better performance in\n",
    "    general.\n",
    "\n",
    "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
    "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
    "\n",
    "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
    "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
    "    method. It performs sequential one-dimensional minimizations along\n",
    "    each vector of the directions set (`direc` field in `options` and\n",
    "    `info`), which is updated at each iteration of the main\n",
    "    minimization loop. The function need not be differentiable, and no\n",
    "    derivatives are taken. If bounds are not provided, then an\n",
    "    unbounded line search will be used. If bounds are provided and\n",
    "    the initial guess is within the bounds, then every function\n",
    "    evaluation throughout the minimization procedure will be within\n",
    "    the bounds. If bounds are provided, the initial guess is outside\n",
    "    the bounds, and `direc` is full rank (default has full rank), then\n",
    "    some function evaluations during the first iteration may be\n",
    "    outside the bounds, but every function evaluation after the first\n",
    "    iteration will be within the bounds. If `direc` is not full rank,\n",
    "    then some parameters may not be optimized and the solution is not\n",
    "    guaranteed to be within the bounds.\n",
    "\n",
    "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
    "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
    "    to bounds. This algorithm uses gradient information; it is also\n",
    "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
    "    method described above as it wraps a C implementation and allows\n",
    "    each variable to be given upper and lower bounds.\n",
    "\n",
    "    **Constrained Minimization**\n",
    "\n",
    "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
    "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
    "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
    "    approximations to the objective function and each constraint. The\n",
    "    method wraps a FORTRAN implementation of the algorithm. The\n",
    "    constraints functions 'fun' may return either a single number\n",
    "    or an array or list of numbers.\n",
    "\n",
    "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
    "    Least SQuares Programming to minimize a function of several\n",
    "    variables with any combination of bounds, equality and inequality\n",
    "    constraints. The method wraps the SLSQP Optimization subroutine\n",
    "    originally implemented by Dieter Kraft [12]_. Note that the\n",
    "    wrapper handles infinite values in bounds by converting them into\n",
    "    large floating values.\n",
    "\n",
    "    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
    "    trust-region algorithm for constrained optimization. It swiches\n",
    "    between two implementations depending on the problem definition.\n",
    "    It is the most versatile constrained minimization algorithm\n",
    "    implemented in SciPy and the most appropriate for large-scale problems.\n",
    "    For equality constrained problems it is an implementation of Byrd-Omojokun\n",
    "    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
    "    inequality constraints  are imposed as well, it swiches to the trust-region\n",
    "    interior point  method described in [16]_. This interior point algorithm,\n",
    "    in turn, solves inequality constraints by introducing slack variables\n",
    "    and solving a sequence of equality-constrained barrier problems\n",
    "    for progressively smaller values of the barrier parameter.\n",
    "    The previously described equality constrained SQP method is\n",
    "    used to solve the subproblems with increasing levels of accuracy\n",
    "    as the iterate gets closer to a solution.\n",
    "\n",
    "    **Finite-Difference Options**\n",
    "\n",
    "    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
    "    the gradient and the Hessian may be approximated using\n",
    "    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
    "    The scheme 'cs' is, potentially, the most accurate but it\n",
    "    requires the function to correctly handles complex inputs and to\n",
    "    be differentiable in the complex plane. The scheme '3-point' is more\n",
    "    accurate than '2-point' but requires twice as many operations.\n",
    "\n",
    "    **Custom minimizers**\n",
    "\n",
    "    It may be useful to pass a custom minimization method, for example\n",
    "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
    "    or a different library.  You can simply pass a callable as the ``method``\n",
    "    parameter.\n",
    "\n",
    "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
    "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
    "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
    "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
    "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
    "    `fun` returns just the function values and `jac` is converted to a function\n",
    "    returning the Jacobian.  The method shall return an `OptimizeResult`\n",
    "    object.\n",
    "\n",
    "    The provided `method` callable must be able to accept (and possibly ignore)\n",
    "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
    "    expand in future versions and then these parameters will be passed to\n",
    "    the method.  You can find an example in the scipy.optimize tutorial.\n",
    "\n",
    "    .. versionadded:: 0.11.0\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
    "        Minimization. The Computer Journal 7: 308-13.\n",
    "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
    "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
    "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
    "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
    "        191-208.\n",
    "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
    "       a function of several variables without calculating derivatives. The\n",
    "       Computer Journal 7: 155-162.\n",
    "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
    "       Numerical Recipes (any edition), Cambridge University Press.\n",
    "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
    "       Springer New York.\n",
    "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
    "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
    "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
    "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
    "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
    "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
    "       550-560.\n",
    "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
    "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
    "    .. [9] Powell, M J D. A direct search optimization method that models\n",
    "       the objective and constraint functions by linear interpolation.\n",
    "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
    "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
    "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
    "       calculations. 1998. Acta Numerica 7: 287-336.\n",
    "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
    "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
    "       2007/NA03\n",
    "    .. [12] Kraft, D. A software package for sequential quadratic\n",
    "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
    "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
    "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
    "       Trust region methods. 2000. Siam. pp. 169-200.\n",
    "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
    "       implementation of the GLTR method for iterative solution of\n",
    "       the trust region problem\", :arxiv:`1611.04718`\n",
    "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
    "       Trust-Region Subproblem using the Lanczos Method\",\n",
    "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
    "    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
    "        An interior point algorithm for large-scale nonlinear  programming.\n",
    "        SIAM Journal on Optimization 9.4: 877-900.\n",
    "    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
    "        implementation of an algorithm for large-scale equality constrained\n",
    "        optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
    "    function (and its respective derivatives) is implemented in `rosen`\n",
    "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
    "\n",
    "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
    "\n",
    "    A simple application of the *Nelder-Mead* method is:\n",
    "\n",
    "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
    "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
    "    >>> res.x\n",
    "    array([ 1.,  1.,  1.,  1.,  1.])\n",
    "\n",
    "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
    "    options:\n",
    "\n",
    "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
    "    ...                options={'gtol': 1e-6, 'disp': True})\n",
    "    Optimization terminated successfully.\n",
    "             Current function value: 0.000000\n",
    "             Iterations: 26\n",
    "             Function evaluations: 31\n",
    "             Gradient evaluations: 31\n",
    "    >>> res.x\n",
    "    array([ 1.,  1.,  1.,  1.,  1.])\n",
    "    >>> print(res.message)\n",
    "    Optimization terminated successfully.\n",
    "    >>> res.hess_inv\n",
    "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
    "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
    "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
    "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
    "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
    "\n",
    "\n",
    "    Next, consider a minimization problem with several constraints (namely\n",
    "    Example 16.4 from [5]_). The objective function is:\n",
    "\n",
    "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
    "\n",
    "    There are three constraints defined as:\n",
    "\n",
    "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
    "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
    "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
    "\n",
    "    And variables must be positive, hence the following bounds:\n",
    "\n",
    "    >>> bnds = ((0, None), (0, None))\n",
    "\n",
    "    The optimization problem is solved using the SLSQP method as:\n",
    "\n",
    "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
    "    ...                constraints=cons)\n",
    "\n",
    "    It should converge to the theoretical solution (1.4 ,1.7).\n",
    "\n",
    "    \"\"\"\n",
    "    x0 = np.asarray(x0)\n",
    "    if x0.dtype.kind in np.typecodes[\"AllInteger\"]:\n",
    "        x0 = np.asarray(x0, dtype=float)\n",
    "\n",
    "    if not isinstance(args, tuple):\n",
    "        args = (args,)\n",
    "\n",
    "    if method is None:\n",
    "        # Select automatically\n",
    "        if constraints:\n",
    "            method = 'SLSQP'\n",
    "        elif bounds is not None:\n",
    "            method = 'L-BFGS-B'\n",
    "        else:\n",
    "            method = 'BFGS'\n",
    "\n",
    "    if callable(method):\n",
    "        meth = \"_custom\"\n",
    "    else:\n",
    "        meth = method.lower()\n",
    "\n",
    "    if options is None:\n",
    "        options = {}\n",
    "    # check if optional parameters are supported by the selected method\n",
    "    # - jac\n",
    "    if meth in ('nelder-mead', 'powell', 'cobyla') and bool(jac):\n",
    "        warn('Method %s does not use gradient information (jac).' % method,\n",
    "             RuntimeWarning)\n",
    "    # - hess\n",
    "    if meth not in ('newton-cg', 'dogleg', 'trust-ncg', 'trust-constr',\n",
    "                    'trust-krylov', 'trust-exact', '_custom') and hess is not None:\n",
    "        warn('Method %s does not use Hessian information (hess).' % method,\n",
    "             RuntimeWarning)\n",
    "    # - hessp\n",
    "    if meth not in ('newton-cg', 'dogleg', 'trust-ncg', 'trust-constr',\n",
    "                    'trust-krylov', '_custom') \\\n",
    "       and hessp is not None:\n",
    "        warn('Method %s does not use Hessian-vector product '\n",
    "             'information (hessp).' % method, RuntimeWarning)\n",
    "    # - constraints or bounds\n",
    "    if (meth in ('cg', 'bfgs', 'newton-cg', 'dogleg', 'trust-ncg')\n",
    "            and (bounds is not None or np.any(constraints))):\n",
    "        warn('Method %s cannot handle constraints nor bounds.' % method,\n",
    "             RuntimeWarning)\n",
    "    if meth in ('nelder-mead', 'l-bfgs-b', 'tnc', 'powell') and np.any(constraints):\n",
    "        warn('Method %s cannot handle constraints.' % method,\n",
    "             RuntimeWarning)\n",
    "    if meth == 'cobyla' and bounds is not None:\n",
    "        warn('Method %s cannot handle bounds.' % method,\n",
    "             RuntimeWarning)\n",
    "    # - callback\n",
    "    if (meth in ('cobyla',) and callback is not None):\n",
    "        warn('Method %s does not support callback.' % method, RuntimeWarning)\n",
    "    # - return_all\n",
    "    if (meth in ('l-bfgs-b', 'tnc', 'cobyla', 'slsqp') and\n",
    "            options.get('return_all', False)):\n",
    "        warn('Method %s does not support the return_all option.' % method,\n",
    "             RuntimeWarning)\n",
    "\n",
    "    # check gradient vector\n",
    "    if callable(jac):\n",
    "        pass\n",
    "    elif jac is True:\n",
    "        # fun returns func and grad\n",
    "        fun = MemoizeJac(fun)\n",
    "        jac = fun.derivative\n",
    "    elif (jac in FD_METHODS and\n",
    "          meth in ['trust-constr', 'bfgs', 'cg', 'l-bfgs-b', 'tnc', 'slsqp']):\n",
    "        # finite differences with relative step\n",
    "        pass\n",
    "    elif meth in ['trust-constr']:\n",
    "        # default jac calculation for this method\n",
    "        jac = '2-point'\n",
    "    elif jac is None or bool(jac) is False:\n",
    "        # this will cause e.g. LBFGS to use forward difference, absolute step\n",
    "        jac = None\n",
    "    else:\n",
    "        # default if jac option is not understood\n",
    "        jac = None\n",
    "\n",
    "    # set default tolerances\n",
    "    if tol is not None:\n",
    "        options = dict(options)\n",
    "        if meth == 'nelder-mead':\n",
    "            options.setdefault('xatol', tol)\n",
    "            options.setdefault('fatol', tol)\n",
    "        if meth in ('newton-cg', 'powell', 'tnc'):\n",
    "            options.setdefault('xtol', tol)\n",
    "        if meth in ('powell', 'l-bfgs-b', 'tnc', 'slsqp'):\n",
    "            options.setdefault('ftol', tol)\n",
    "        if meth in ('bfgs', 'cg', 'l-bfgs-b', 'tnc', 'dogleg',\n",
    "                    'trust-ncg', 'trust-exact', 'trust-krylov'):\n",
    "            options.setdefault('gtol', tol)\n",
    "        if meth in ('cobyla', '_custom'):\n",
    "            options.setdefault('tol', tol)\n",
    "        if meth == 'trust-constr':\n",
    "            options.setdefault('xtol', tol)\n",
    "            options.setdefault('gtol', tol)\n",
    "            options.setdefault('barrier_tol', tol)\n",
    "\n",
    "    if meth == '_custom':\n",
    "        # custom method called before bounds and constraints are 'standardised'\n",
    "        # custom method should be able to accept whatever bounds/constraints\n",
    "        # are provided to it.\n",
    "        return method(fun, x0, args=args, jac=jac, hess=hess, hessp=hessp,\n",
    "                      bounds=bounds, constraints=constraints,\n",
    "                      callback=callback, **options)\n",
    "\n",
    "    if bounds is not None:\n",
    "        bounds = standardize_bounds(bounds, x0, meth)\n",
    "\n",
    "    if constraints is not None:\n",
    "        constraints = standardize_constraints(constraints, x0, meth)\n",
    "\n",
    "    if meth == 'nelder-mead':\n",
    "        return _minimize_neldermead(fun, x0, args, callback, bounds=bounds,\n",
    "                                    **options)\n",
    "    elif meth == 'powell':\n",
    "        return _minimize_powell(fun, x0, args, callback, bounds, **options)\n",
    "    elif meth == 'cg':\n",
    "        return _minimize_cg(fun, x0, args, jac, callback, **options)\n",
    "    elif meth == 'bfgs':\n",
    "        return _minimize_bfgs(fun, x0, args, jac, callback, **options)\n",
    "    elif meth == 'newton-cg':\n",
    "        return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
    "                                  **options)\n",
    "    elif meth == 'l-bfgs-b':\n",
    "        return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
    "                                callback=callback, **options)\n",
    "    elif meth == 'tnc':\n",
    "        return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
    "                             **options)\n",
    "    elif meth == 'cobyla':\n",
    "        return _minimize_cobyla(fun, x0, args, constraints, **options)\n",
    "    elif meth == 'slsqp':\n",
    "        return _minimize_slsqp(fun, x0, args, jac, bounds,\n",
    "                               constraints, callback=callback, **options)\n",
    "    elif meth == 'trust-constr':\n",
    "        return _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n",
    "                                            bounds, constraints,\n",
    "                                            callback=callback, **options)\n",
    "    elif meth == 'dogleg':\n",
    "        return _minimize_dogleg(fun, x0, args, jac, hess,\n",
    "                                callback=callback, **options)\n",
    "    elif meth == 'trust-ncg':\n",
    "        return _minimize_trust_ncg(fun, x0, args, jac, hess, hessp,\n",
    "                                   callback=callback, **options)\n",
    "    elif meth == 'trust-krylov':\n",
    "        return _minimize_trust_krylov(fun, x0, args, jac, hess, hessp,\n",
    "                                      callback=callback, **options)\n",
    "    elif meth == 'trust-exact':\n",
    "        return _minimize_trustregion_exact(fun, x0, args, jac, hess,\n",
    "                                           callback=callback, **options)\n",
    "    else:\n",
    "        raise ValueError('Unknown solver %s' % method)\n",
    "\n",
    "\n",
    "def minimize_scalar(fun, bracket=None, bounds=None, args=(),\n",
    "                    method='brent', tol=None, options=None):\n",
    "    \"\"\"Minimization of scalar function of one variable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fun : callable\n",
    "        Objective function.\n",
    "        Scalar function, must return a scalar.\n",
    "    bracket : sequence, optional\n",
    "        For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
    "        interval and can either have three items ``(a, b, c)`` so that\n",
    "        ``a < b < c`` and ``fun(b) < fun(a), fun(c)`` or two items ``a`` and\n",
    "        ``c`` which are assumed to be a starting interval for a downhill\n",
    "        bracket search (see `bracket`); it doesn't always mean that the\n",
    "        obtained solution will satisfy ``a <= x <= c``.\n",
    "    bounds : sequence, optional\n",
    "        For method 'bounded', `bounds` is mandatory and must have two items\n",
    "        corresponding to the optimization bounds.\n",
    "    args : tuple, optional\n",
    "        Extra arguments passed to the objective function.\n",
    "    method : str or callable, optional\n",
    "        Type of solver.  Should be one of:\n",
    "\n",
    "            - 'Brent'     :ref:`(see here) <optimize.minimize_scalar-brent>`\n",
    "            - 'Bounded'   :ref:`(see here) <optimize.minimize_scalar-bounded>`\n",
    "            - 'Golden'    :ref:`(see here) <optimize.minimize_scalar-golden>`\n",
    "            - custom - a callable object (added in version 0.14.0), see below\n",
    "\n",
    "    tol : float, optional\n",
    "        Tolerance for termination. For detailed control, use solver-specific\n",
    "        options.\n",
    "    options : dict, optional\n",
    "        A dictionary of solver options.\n",
    "\n",
    "            maxiter : int\n",
    "                Maximum number of iterations to perform.\n",
    "            disp : bool\n",
    "                Set to True to print convergence messages.\n",
    "\n",
    "        See :func:`show_options()` for solver-specific options.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : OptimizeResult\n",
    "        The optimization result represented as a ``OptimizeResult`` object.\n",
    "        Important attributes are: ``x`` the solution array, ``success`` a\n",
    "        Boolean flag indicating if the optimizer exited successfully and\n",
    "        ``message`` which describes the cause of the termination. See\n",
    "        `OptimizeResult` for a description of other attributes.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    minimize : Interface to minimization algorithms for scalar multivariate\n",
    "        functions\n",
    "    show_options : Additional options accepted by the solvers\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This section describes the available solvers that can be selected by the\n",
    "    'method' parameter. The default method is *Brent*.\n",
    "\n",
    "    Method :ref:`Brent <optimize.minimize_scalar-brent>` uses Brent's\n",
    "    algorithm to find a local minimum.  The algorithm uses inverse\n",
    "    parabolic interpolation when possible to speed up convergence of\n",
    "    the golden section method.\n",
    "\n",
    "    Method :ref:`Golden <optimize.minimize_scalar-golden>` uses the\n",
    "    golden section search technique. It uses analog of the bisection\n",
    "    method to decrease the bracketed interval. It is usually\n",
    "    preferable to use the *Brent* method.\n",
    "\n",
    "    Method :ref:`Bounded <optimize.minimize_scalar-bounded>` can\n",
    "    perform bounded minimization. It uses the Brent method to find a\n",
    "    local minimum in the interval x1 < xopt < x2.\n",
    "\n",
    "    **Custom minimizers**\n",
    "\n",
    "    It may be useful to pass a custom minimization method, for example\n",
    "    when using some library frontend to minimize_scalar. You can simply\n",
    "    pass a callable as the ``method`` parameter.\n",
    "\n",
    "    The callable is called as ``method(fun, args, **kwargs, **options)``\n",
    "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
    "    (such as `bracket`, `tol`, etc.), except the `options` dict, which has\n",
    "    its contents also passed as `method` parameters pair by pair.  The method\n",
    "    shall return an `OptimizeResult` object.\n",
    "\n",
    "    The provided `method` callable must be able to accept (and possibly ignore)\n",
    "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
    "    expand in future versions and then these parameters will be passed to\n",
    "    the method. You can find an example in the scipy.optimize tutorial.\n",
    "\n",
    "    .. versionadded:: 0.11.0\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Consider the problem of minimizing the following function.\n",
    "\n",
    "    >>> def f(x):\n",
    "    ...     return (x - 2) * x * (x + 2)**2\n",
    "\n",
    "    Using the *Brent* method, we find the local minimum as:\n",
    "\n",
    "    >>> from scipy.optimize import minimize_scalar\n",
    "    >>> res = minimize_scalar(f)\n",
    "    >>> res.x\n",
    "    1.28077640403\n",
    "\n",
    "    Using the *Bounded* method, we find a local minimum with specified\n",
    "    bounds as:\n",
    "\n",
    "    >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
    "    >>> res.x\n",
    "    -2.0000002026\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(args, tuple):\n",
    "        args = (args,)\n",
    "\n",
    "    if callable(method):\n",
    "        meth = \"_custom\"\n",
    "    else:\n",
    "        meth = method.lower()\n",
    "    if options is None:\n",
    "        options = {}\n",
    "\n",
    "    if tol is not None:\n",
    "        options = dict(options)\n",
    "        if meth == 'bounded' and 'xatol' not in options:\n",
    "            warn(\"Method 'bounded' does not support relative tolerance in x; \"\n",
    "                 \"defaulting to absolute tolerance.\", RuntimeWarning)\n",
    "            options['xatol'] = tol\n",
    "        elif meth == '_custom':\n",
    "            options.setdefault('tol', tol)\n",
    "        else:\n",
    "            options.setdefault('xtol', tol)\n",
    "\n",
    "    if meth == '_custom':\n",
    "        return method(fun, args=args, bracket=bracket, bounds=bounds, **options)\n",
    "    elif meth == 'brent':\n",
    "        return _minimize_scalar_brent(fun, bracket, args, **options)\n",
    "    elif meth == 'bounded':\n",
    "        if bounds is None:\n",
    "            raise ValueError('The `bounds` parameter is mandatory for '\n",
    "                             'method `bounded`.')\n",
    "        # replace boolean \"disp\" option, if specified, by an integer value, as\n",
    "        # expected by _minimize_scalar_bounded()\n",
    "        disp = options.get('disp')\n",
    "        if isinstance(disp, bool):\n",
    "            options['disp'] = 2 * int(disp)\n",
    "        return _minimize_scalar_bounded(fun, bounds, args, **options)\n",
    "    elif meth == 'golden':\n",
    "        return _minimize_scalar_golden(fun, bracket, args, **options)\n",
    "    else:\n",
    "        raise ValueError('Unknown solver %s' % method)\n",
    "\n",
    "\n",
    "def standardize_bounds(bounds, x0, meth):\n",
    "    \"\"\"Converts bounds to the form required by the solver.\"\"\"\n",
    "    if meth in {'trust-constr', 'powell', 'nelder-mead'}:\n",
    "        if not isinstance(bounds, Bounds):\n",
    "            lb, ub = old_bound_to_new(bounds)\n",
    "            bounds = Bounds(lb, ub)\n",
    "    elif meth in ('l-bfgs-b', 'tnc', 'slsqp'):\n",
    "        if isinstance(bounds, Bounds):\n",
    "            bounds = new_bounds_to_old(bounds.lb, bounds.ub, x0.shape[0])\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def standardize_constraints(constraints, x0, meth):\n",
    "    \"\"\"Converts constraints to the form required by the solver.\"\"\"\n",
    "    all_constraint_types = (NonlinearConstraint, LinearConstraint, dict)\n",
    "    new_constraint_types = all_constraint_types[:-1]\n",
    "    if isinstance(constraints, all_constraint_types):\n",
    "        constraints = [constraints]\n",
    "    constraints = list(constraints)  # ensure it's a mutable sequence\n",
    "\n",
    "    if meth == 'trust-constr':\n",
    "        for i, con in enumerate(constraints):\n",
    "            if not isinstance(con, new_constraint_types):\n",
    "                constraints[i] = old_constraint_to_new(i, con)\n",
    "    else:\n",
    "        # iterate over copy, changing original\n",
    "        for i, con in enumerate(list(constraints)):\n",
    "            if isinstance(con, new_constraint_types):\n",
    "                old_constraints = new_constraint_to_old(con, x0)\n",
    "                constraints[i] = old_constraints[0]\n",
    "                constraints.extend(old_constraints[1:])  # appends 1 if present\n",
    "\n",
    "    return constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2066042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate text archive files containing parameter values and mse values\n",
    "with open('parameters_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'Ea'+'\\t'+'Ploading'+'\\n')\n",
    "with open('mse_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'MSE'+'\\n')\n",
    "    \n",
    "# initialize count variable to count up number of parameter sets\n",
    "count = 1\n",
    "\n",
    "# call optimization function\n",
    "minimize(fun, x0=[0.6,20], method='Powell', bounds=[(0,0.6),(0,20)], options = {'xtol':0.001, 'ftol':0.001, 'maxiter':1, 'return_all':True, 'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate text archive files containing parameter values and mse values\n",
    "with open('parameters_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'Ea'+'\\t'+'Ploading'+'\\n')\n",
    "with open('mse_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'MSE'+'\\n')\n",
    "\n",
    "# initialize count variable to count up number of parameter sets\n",
    "count = 1\n",
    "\n",
    "# call optimization function\n",
    "scipy.optimize.minimize(fun, x0=[0.6,20], method='Powell', bounds=[(0,0.6),(0,20)], options = {'xtol':0.001, 'ftol':0.001, 'maxiter':1, 'maxfev':10, 'return_all':True, 'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569861d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate text archive files containing parameter values and mse values\n",
    "with open('parameters_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'Ea'+'\\t'+'Ploading'+'\\n')\n",
    "with open('mse_archive.txt','w') as file:\n",
    "    file.writelines('count'+'\\t'+'MSE'+'\\n')\n",
    "\n",
    "# initialize count variable to count up number of parameter sets\n",
    "count = 1\n",
    "\n",
    "itr = 1\n",
    "def callbackF(x):\n",
    "    global itr\n",
    "    print(count,x[0],x[1])\n",
    "    itr += 1\n",
    "    \n",
    "# call optimization function\n",
    "scipy.optimize.minimize(fun, x0=[0.6,20], method='Powell', bounds=[(0,0.6),(0,20)], tol=0.001, callback=callbackF, options = {'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dced8f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load Ea and Ploading values\n",
    "with open('parameters_archive_2.txt', 'r') as file:\n",
    "    content = file.readlines()\n",
    "#data = []\n",
    "Ea = []\n",
    "Ploading = []\n",
    "for line in content:\n",
    "    if len(line.split()) == 4:\n",
    "        E = float(line.split()[3])\n",
    "        #data.append(E)\n",
    "        Ea.append(E)\n",
    "    elif len(line.split()) == 3:\n",
    "        P = float(line.split()[2])\n",
    "        #data.append(P)\n",
    "        Ploading.append(P)\n",
    "#data = np.array(data).reshape(-1,2)\n",
    "Ea = np.array(Ea)\n",
    "Ea = Ea[:-1]\n",
    "Ploading = np.array(Ploading)\n",
    "Ploading = Ploading[:-1]\n",
    "\n",
    "# load MSE values\n",
    "with open('mse_archive_2.txt','r') as file:\n",
    "    content = file.readlines()\n",
    "MSE = []\n",
    "for line in content:\n",
    "    MSE.append(float(line.split()[1]))\n",
    "MSE = np.array(MSE)\n",
    "\n",
    "i = 0\n",
    "def fun(x):\n",
    "    global MSE,i\n",
    "    i += 1\n",
    "    print(i,x[0],x[1],MSE[i-1])\n",
    "    return MSE[i-1]\n",
    "\n",
    "count = 1\n",
    "def callbackF(x):\n",
    "    global count\n",
    "    print(count,x[0],x[1])\n",
    "    count += 1\n",
    "    \n",
    "scipy.optimize.minimize(fun, x0=[0.6,20], method='Powell', bounds=[(0,0.6),(0,20)], tol=0.001, callback=callbackF, options = {'maxiter':5, 'return_all':True, 'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbf79a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "A = []\n",
    "B = []\n",
    "func = []\n",
    "def fun(x):\n",
    "    global A,B,func,count\n",
    "    a, b = x\n",
    "    A.append(a)\n",
    "    B.append(b)\n",
    "    print(count,a,b,abs(a**2 - 3*b**2))\n",
    "    func.append(a**2 + b**2)\n",
    "    count += 1\n",
    "    return abs(a**2 - 3*b**2)\n",
    "\n",
    "itr = 1\n",
    "def callbackF(x):\n",
    "    global itr,count\n",
    "    print('##############################',itr,x[0],x[1],'##############################')#,x[0]**2+x[1]**2)\n",
    "    itr += 1\n",
    "\n",
    "minimize(fun, x0=(0.6,20), method='Powell', bounds=((0,0.6),(0,20),), callback=callbackF, options = {'ftol':0.01, 'return_all':True, 'disp':True})\n",
    "\n",
    "A = np.array(A)\n",
    "B = np.array(B)\n",
    "func = np.array(func)\n",
    "\n",
    "t=np.arange(len(A))\n",
    "\n",
    "fig = plt.figure(figsize=(10,8), dpi=80)\n",
    "plt.plot(t,func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "def fun(x):\n",
    "    global count\n",
    "    Ea, Ploading = x\n",
    "    if Ea < 0:\n",
    "        #Ea = 0\n",
    "        print(\"#################################################\")\n",
    "    if Ea > 0.6:\n",
    "        #Ea = 0.6\n",
    "        print(\"#################################################\")\n",
    "    if Ploading < 0:\n",
    "        #Ploading = 0\n",
    "        print(\"#################################################\")\n",
    "    if Ploading > 20:\n",
    "        #Ploading = 20\n",
    "        print(\"#################################################\")\n",
    "    print(count)\n",
    "    print(x)\n",
    "    print(Ea**2 + Ploading**2)\n",
    "    count += 1\n",
    "    return Ea**2 + Ploading**2\n",
    "scipy.optimize.minimize(fun, x0=(0,20), method='Powell', bounds=((0,0.6),(0,20)), options = {'xtol':0.001, 'ftol':0.001, 'return_all':True, 'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(scipy.optimize.minimize))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
